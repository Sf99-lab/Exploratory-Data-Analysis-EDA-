{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4139805,"sourceType":"datasetVersion","datasetId":2445309}],"dockerImageVersionId":30260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-22T12:24:23.747738Z","iopub.execute_input":"2024-07-22T12:24:23.748181Z","iopub.status.idle":"2024-07-22T12:24:23.765723Z","shell.execute_reply.started":"2024-07-22T12:24:23.748142Z","shell.execute_reply":"2024-07-22T12:24:23.764580Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/bank-customer-churn-dataset/Bank Customer Churn Prediction.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Enhancements to Customer Retention Analysis Notebook\n\nThis notebook builds upon the original analysis by adding the following:\n- Additional feature engineering steps.\n- Improved data visualizations.\n- Hyperparameter tuning for the machine learning model.\n- Detailed explanation of results.\n","metadata":{}},{"cell_type":"markdown","source":"# **1. INTRODUCTION**","metadata":{}},{"cell_type":"markdown","source":"**1. customer_id:** This is a unique identifier for each customer. It is mentioned as an unused variable, meaning it is not considered in the analysis or model building.\n\n**2. credit_score:** This represents the credit score of the customer, which is used as an input feature in the analysis or model.\n\n**3. country:** This indicates the country of residence of the customer and is used as an input feature.\n\n**4. gender:** This specifies the gender of the customer (e.g., male or female) and is used as an input feature.\n\n**5. age:** This represents the age of the customer and is used as an input feature.\n\n**6. tenure:** This indicates the number of years the customer has been with the bank and is used as an input feature.\n\n**7. balance:** This represents the current balance in the customer's account and is used as an input feature.\n\n**8. products_number:** This shows the number of products the customer has with the bank (e.g., loans, credit cards) and is used as an input feature.\n\n**9. credit_card:** This is a binary variable indicating whether the customer has a credit card (1) or not (0). It is used as an input feature.\n\n**10. active_member:** This is a binary variable indicating whether the customer is an active member (1) or not (0). It is used as an input feature.\n\n**11. estimated_salary:** This represents the estimated annual salary of the customer and is used as an input feature.\n\n**12. churn:** This is the target variable. It is binary, where 1 indicates that the customer has left the bank during a certain period, and 0 indicates that the customer has not left the bank.\n\nIn summary, most columns are input features used to predict the target variable, which is to **Predict the Customer Churn for ABC Bank**. The customer_id is an identifier and is not used in the predictive analysis.","metadata":{}},{"cell_type":"markdown","source":"# 2. LOADING PAKAGES AND DATA\n***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-07-22T12:42:19.386514Z","iopub.execute_input":"2024-07-22T12:42:19.387362Z","iopub.status.idle":"2024-07-22T12:42:19.397283Z","shell.execute_reply.started":"2024-07-22T12:42:19.387320Z","shell.execute_reply":"2024-07-22T12:42:19.395946Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 3. EXPLORATORY DATA ANALYSIS\n***","metadata":{}},{"cell_type":"code","source":"bank_cust_df = pd.read_csv(\"/kaggle/input/bank-customer-churn-dataset/Bank Customer Churn Prediction.csv\")\nbank_cust_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T12:43:59.563234Z","iopub.execute_input":"2024-07-22T12:43:59.564039Z","iopub.status.idle":"2024-07-22T12:43:59.621262Z","shell.execute_reply.started":"2024-07-22T12:43:59.563996Z","shell.execute_reply":"2024-07-22T12:43:59.620073Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   customer_id  credit_score country  gender  age  tenure    balance  \\\n0     15634602           619  France  Female   42       2       0.00   \n1     15647311           608   Spain  Female   41       1   83807.86   \n2     15619304           502  France  Female   42       8  159660.80   \n3     15701354           699  France  Female   39       1       0.00   \n4     15737888           850   Spain  Female   43       2  125510.82   \n\n   products_number  credit_card  active_member  estimated_salary  churn  \n0                1            1              1         101348.88      1  \n1                1            0              1         112542.58      0  \n2                3            1              0         113931.57      1  \n3                2            0              0          93826.63      0  \n4                1            1              1          79084.10      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>credit_score</th>\n      <th>country</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>tenure</th>\n      <th>balance</th>\n      <th>products_number</th>\n      <th>credit_card</th>\n      <th>active_member</th>\n      <th>estimated_salary</th>\n      <th>churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15634602</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15647311</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15619304</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15701354</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15737888</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\nJust by looking at the dataset I think that as we are only intrested in predicting the people leaving the bank or not, columns like custimer_id and credit score dosen't help.\n\nWe can further remove those columns.\n\nBut as I say, I think so, that dose not mean it has to be it. \n\nSo before comming to any conclusion let's explore our data even more.","metadata":{}},{"cell_type":"code","source":"#descriptive statistics summary\nbank_cust_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T12:26:26.715850Z","iopub.execute_input":"2024-07-22T12:26:26.716322Z","iopub.status.idle":"2024-07-22T12:26:26.774301Z","shell.execute_reply.started":"2024-07-22T12:26:26.716284Z","shell.execute_reply":"2024-07-22T12:26:26.773216Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        customer_id  credit_score           age        tenure        balance  \\\ncount  1.000000e+04  10000.000000  10000.000000  10000.000000   10000.000000   \nmean   1.569094e+07    650.528800     38.921800      5.012800   76485.889288   \nstd    7.193619e+04     96.653299     10.487806      2.892174   62397.405202   \nmin    1.556570e+07    350.000000     18.000000      0.000000       0.000000   \n25%    1.562853e+07    584.000000     32.000000      3.000000       0.000000   \n50%    1.569074e+07    652.000000     37.000000      5.000000   97198.540000   \n75%    1.575323e+07    718.000000     44.000000      7.000000  127644.240000   \nmax    1.581569e+07    850.000000     92.000000     10.000000  250898.090000   \n\n       products_number  credit_card  active_member  estimated_salary  \\\ncount     10000.000000  10000.00000   10000.000000      10000.000000   \nmean          1.530200      0.70550       0.515100     100090.239881   \nstd           0.581654      0.45584       0.499797      57510.492818   \nmin           1.000000      0.00000       0.000000         11.580000   \n25%           1.000000      0.00000       0.000000      51002.110000   \n50%           1.000000      1.00000       1.000000     100193.915000   \n75%           2.000000      1.00000       1.000000     149388.247500   \nmax           4.000000      1.00000       1.000000     199992.480000   \n\n              churn  \ncount  10000.000000  \nmean       0.203700  \nstd        0.402769  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>credit_score</th>\n      <th>age</th>\n      <th>tenure</th>\n      <th>balance</th>\n      <th>products_number</th>\n      <th>credit_card</th>\n      <th>active_member</th>\n      <th>estimated_salary</th>\n      <th>churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.000000e+04</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.00000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.569094e+07</td>\n      <td>650.528800</td>\n      <td>38.921800</td>\n      <td>5.012800</td>\n      <td>76485.889288</td>\n      <td>1.530200</td>\n      <td>0.70550</td>\n      <td>0.515100</td>\n      <td>100090.239881</td>\n      <td>0.203700</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.193619e+04</td>\n      <td>96.653299</td>\n      <td>10.487806</td>\n      <td>2.892174</td>\n      <td>62397.405202</td>\n      <td>0.581654</td>\n      <td>0.45584</td>\n      <td>0.499797</td>\n      <td>57510.492818</td>\n      <td>0.402769</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.556570e+07</td>\n      <td>350.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>11.580000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.562853e+07</td>\n      <td>584.000000</td>\n      <td>32.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>51002.110000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.569074e+07</td>\n      <td>652.000000</td>\n      <td>37.000000</td>\n      <td>5.000000</td>\n      <td>97198.540000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>100193.915000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.575323e+07</td>\n      <td>718.000000</td>\n      <td>44.000000</td>\n      <td>7.000000</td>\n      <td>127644.240000</td>\n      <td>2.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>149388.247500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.581569e+07</td>\n      <td>850.000000</td>\n      <td>92.000000</td>\n      <td>10.000000</td>\n      <td>250898.090000</td>\n      <td>4.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>199992.480000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"bank_cust_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:54.044285Z","iopub.execute_input":"2022-10-12T09:42:54.044719Z","iopub.status.idle":"2022-10-12T09:42:54.053355Z","shell.execute_reply.started":"2022-10-12T09:42:54.044686Z","shell.execute_reply":"2022-10-12T09:42:54.051873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:54.129365Z","iopub.execute_input":"2022-10-12T09:42:54.130466Z","iopub.status.idle":"2022-10-12T09:42:54.142404Z","shell.execute_reply.started":"2022-10-12T09:42:54.130422Z","shell.execute_reply":"2022-10-12T09:42:54.141146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:54.248202Z","iopub.execute_input":"2022-10-12T09:42:54.24864Z","iopub.status.idle":"2022-10-12T09:42:54.264589Z","shell.execute_reply.started":"2022-10-12T09:42:54.248595Z","shell.execute_reply":"2022-10-12T09:42:54.263492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\n**Observation so far**\n\nBy doing some of the above steps I can conclude some points as,\n1. There is no null values in data set, which is quite good for us\n2. There are 9 quantitaive feature and 2 qualitative feature  ","metadata":{}},{"cell_type":"markdown","source":"### Now let's do some visualization","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nbank_cust_df.hist(bins=50, figsize=(20,15))\n# x-axis is column Values and Y-axis is Total Counts\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:54.271888Z","iopub.execute_input":"2022-10-12T09:42:54.272369Z","iopub.status.idle":"2022-10-12T09:42:56.481342Z","shell.execute_reply.started":"2022-10-12T09:42:54.272328Z","shell.execute_reply":"2022-10-12T09:42:56.479886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.plot(kind=\"scatter\", x=\"balance\", y=\"age\", alpha=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:56.483746Z","iopub.execute_input":"2022-10-12T09:42:56.484779Z","iopub.status.idle":"2022-10-12T09:42:56.769939Z","shell.execute_reply.started":"2022-10-12T09:42:56.484735Z","shell.execute_reply":"2022-10-12T09:42:56.768639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.plot(kind=\"scatter\", x=\"balance\", y=\"estimated_salary\", alpha=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:56.771732Z","iopub.execute_input":"2022-10-12T09:42:56.772092Z","iopub.status.idle":"2022-10-12T09:42:58.18251Z","shell.execute_reply.started":"2022-10-12T09:42:56.772059Z","shell.execute_reply":"2022-10-12T09:42:58.181197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.plot(kind=\"scatter\", x=\"age\", y=\"estimated_salary\", alpha=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:58.18537Z","iopub.execute_input":"2022-10-12T09:42:58.185961Z","iopub.status.idle":"2022-10-12T09:42:58.487231Z","shell.execute_reply.started":"2022-10-12T09:42:58.185924Z","shell.execute_reply":"2022-10-12T09:42:58.486186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = bank_cust_df.corr()\nfig , ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(8)\nsns.heatmap(correlation,annot=True,cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:58.48865Z","iopub.execute_input":"2022-10-12T09:42:58.489227Z","iopub.status.idle":"2022-10-12T09:42:59.401359Z","shell.execute_reply.started":"2022-10-12T09:42:58.489189Z","shell.execute_reply":"2022-10-12T09:42:59.400393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\n**Observation so far**\n\nEverything looks good so far, but I think there is almost no correlation between any variables.\n\nThough I got a bit shock by looking at some scatter plots where it says,\n1. Most of the people range from age 20 to 90 has zero balance in there accountsü§¶‚Äç\n2. People has lot salary but don't have balance in there account, which made me think that what are they doing with their salaries?ü§î\n3. Most of the people range from age 20 to 90 has has zero salary, aren't they working?üòí and that might be the reason for not having balance in thier accounts.","metadata":{}},{"cell_type":"code","source":"bank_cust_df['gender'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.402759Z","iopub.execute_input":"2022-10-12T09:42:59.403304Z","iopub.status.idle":"2022-10-12T09:42:59.412651Z","shell.execute_reply.started":"2022-10-12T09:42:59.40327Z","shell.execute_reply":"2022-10-12T09:42:59.411358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df['country'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.414364Z","iopub.execute_input":"2022-10-12T09:42:59.414868Z","iopub.status.idle":"2022-10-12T09:42:59.429414Z","shell.execute_reply.started":"2022-10-12T09:42:59.414822Z","shell.execute_reply":"2022-10-12T09:42:59.428133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\n**Observation so far**\n    \n\nI can see that 2 category in \"gender\" column and 3 in \"country\" column.\n\nI can use Nominal encoding to encode them as they don't have any order or prefrences.","metadata":{}},{"cell_type":"markdown","source":"# 3. DATA PREPROCESSING\n***","metadata":{}},{"cell_type":"code","source":"bank_cust_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.43096Z","iopub.execute_input":"2022-10-12T09:42:59.431332Z","iopub.status.idle":"2022-10-12T09:42:59.44458Z","shell.execute_reply.started":"2022-10-12T09:42:59.431302Z","shell.execute_reply":"2022-10-12T09:42:59.443097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\nNow, I think i have learner enough about my data so now let quicky do some of the preprocessing and train our model,\n\nFrom our analysis so far, I found out the preprocessing needed on our data are as,\n1. Delete unwanted columns such as \"customer_id\" and \"credit_score\"\n2. Convert catergorical variable such as \"country\" and \"gender\" into numerical manually because we has very less categories\n3. Performing test-train split \n4. Scaling the dataset using Standart Scaller on test and train datasets simultaniously","metadata":{}},{"cell_type":"markdown","source":"#### 1. Delete unwanted columns such as \"customer_id\" and \"credit_score\"","metadata":{}},{"cell_type":"code","source":"#Deleteing unwanted columns\nbank_cust_df.drop(['customer_id', 'credit_score'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.448945Z","iopub.execute_input":"2022-10-12T09:42:59.449423Z","iopub.status.idle":"2022-10-12T09:42:59.457825Z","shell.execute_reply.started":"2022-10-12T09:42:59.449382Z","shell.execute_reply":"2022-10-12T09:42:59.456523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.461971Z","iopub.execute_input":"2022-10-12T09:42:59.462407Z","iopub.status.idle":"2022-10-12T09:42:59.47327Z","shell.execute_reply.started":"2022-10-12T09:42:59.46237Z","shell.execute_reply":"2022-10-12T09:42:59.472079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Convert catergorical variable such as \"country\" and \"gender\" into numerical manually because we has very less categories","metadata":{}},{"cell_type":"code","source":"#Convert catergorical variable such as \"country\" and \"gender\" into numerical\nbank_cust_df['country'] = bank_cust_df['country'].map({'France':0, 'Germany':1 ,'Spain':2})\nbank_cust_df['gender'] = bank_cust_df['gender'].map({'Male':0, 'Female':1})","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.474348Z","iopub.execute_input":"2022-10-12T09:42:59.47481Z","iopub.status.idle":"2022-10-12T09:42:59.490655Z","shell.execute_reply.started":"2022-10-12T09:42:59.474773Z","shell.execute_reply":"2022-10-12T09:42:59.489432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_cust_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.492446Z","iopub.execute_input":"2022-10-12T09:42:59.492928Z","iopub.status.idle":"2022-10-12T09:42:59.517998Z","shell.execute_reply.started":"2022-10-12T09:42:59.492876Z","shell.execute_reply":"2022-10-12T09:42:59.516712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Performing test-train split ","metadata":{}},{"cell_type":"code","source":"features = bank_cust_df.drop(\"churn\", axis =1)\nX_train, X_test, y_train, y_test = train_test_split(features,bank_cust_df[\"churn\"],test_size=0.20,random_state=10)\nprint(f\"X_train data is {X_train.shape}\")\nprint(f\"y_train data is {y_train.shape}\")\nprint(f\"X_test data is {X_test.shape}\")\nprint(f\"y_test data is {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.519453Z","iopub.execute_input":"2022-10-12T09:42:59.51988Z","iopub.status.idle":"2022-10-12T09:42:59.53609Z","shell.execute_reply.started":"2022-10-12T09:42:59.519842Z","shell.execute_reply":"2022-10-12T09:42:59.534796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Scaling the dataset using Standart Scaller on test and train datasets simultaniously","metadata":{}},{"cell_type":"code","source":"# Perfroming the scaling for Train Set\n# joining X_train and y_train for scaling training set\ntrain_set = X_train.join(pd.DataFrame(y_train))\ntrain_set.columns\n\n# performing scaling method\nscaler = StandardScaler()\nmodel = scaler.fit(train_set)\nscaled_data = model.transform(train_set)\ntrain_tr = pd.DataFrame(scaled_data)\ntrain_tr.columns = ['country', 'gender', 'age', 'tenure', 'balance', 'products_number',\n       'credit_card', 'active_member', 'estimated_salary', 'churn']\ntrain_tr.describe()\n\n# separating scaled X_train and y_train from training set\nX_train_tr = train_tr.drop(\"churn\", axis=1) \ny_train_tr = train_tr[\"churn\"]","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.537728Z","iopub.execute_input":"2022-10-12T09:42:59.539064Z","iopub.status.idle":"2022-10-12T09:42:59.587524Z","shell.execute_reply.started":"2022-10-12T09:42:59.539001Z","shell.execute_reply":"2022-10-12T09:42:59.58631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perfroming the scaling for Test Set\n# joining X_test and y_test for scaling training set\ntest_set = X_test.join(pd.DataFrame(y_test))\ntest_set.columns\n\n# performing scaling method\nscaler = StandardScaler()\nmodel = scaler.fit(test_set)\nscaled_data = model.transform(test_set)\ntest_tr = pd.DataFrame(scaled_data)\ntest_tr.columns = ['country', 'gender', 'age', 'tenure', 'balance', 'products_number',\n       'credit_card', 'active_member', 'estimated_salary', 'churn']\ntest_tr.describe()\n\n# separating scaled X_test and y_test from testing set\nX_test_tr = test_tr.drop(\"churn\", axis=1) \ny_test_tr = test_tr[\"churn\"]","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.589656Z","iopub.execute_input":"2022-10-12T09:42:59.590159Z","iopub.status.idle":"2022-10-12T09:42:59.634384Z","shell.execute_reply.started":"2022-10-12T09:42:59.59008Z","shell.execute_reply":"2022-10-12T09:42:59.63299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. MODEL CREATION AND PERFORMANCE\n***","metadata":{}},{"cell_type":"markdown","source":"#### Decision Tree Model without Hyperparameter","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(X_train_tr, y_train)\ntrain_accuracy = model.score(X_test_tr, y_test)\nprint(f\"Accuracy of Decision Tree Model ---> {train_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.63582Z","iopub.execute_input":"2022-10-12T09:42:59.636561Z","iopub.status.idle":"2022-10-12T09:42:59.685206Z","shell.execute_reply.started":"2022-10-12T09:42:59.636524Z","shell.execute_reply":"2022-10-12T09:42:59.684197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(X_test_tr)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.68651Z","iopub.execute_input":"2022-10-12T09:42:59.687291Z","iopub.status.idle":"2022-10-12T09:42:59.694487Z","shell.execute_reply.started":"2022-10-12T09:42:59.687256Z","shell.execute_reply":"2022-10-12T09:42:59.693323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, prediction)\nnp.set_printoptions(precision=2)\ncnf_matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.695885Z","iopub.execute_input":"2022-10-12T09:42:59.696491Z","iopub.status.idle":"2022-10-12T09:42:59.710815Z","shell.execute_reply.started":"2022-10-12T09:42:59.696455Z","shell.execute_reply":"2022-10-12T09:42:59.709195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree Model with Hyperparameter","metadata":{}},{"cell_type":"code","source":"model_hyper = DecisionTreeClassifier(max_depth=10)\nmodel_hyper.fit(X_train_tr, y_train)\ntrain_accuracy_hyper = model_hyper.score(X_test_tr, y_test)\nprint(f\"Accuracy of Decision Tree Model ---> {train_accuracy_hyper}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.712286Z","iopub.execute_input":"2022-10-12T09:42:59.712744Z","iopub.status.idle":"2022-10-12T09:42:59.753893Z","shell.execute_reply.started":"2022-10-12T09:42:59.712712Z","shell.execute_reply":"2022-10-12T09:42:59.752534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_hyper = model.predict(X_test_tr)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.755519Z","iopub.execute_input":"2022-10-12T09:42:59.755959Z","iopub.status.idle":"2022-10-12T09:42:59.764417Z","shell.execute_reply.started":"2022-10-12T09:42:59.755923Z","shell.execute_reply":"2022-10-12T09:42:59.76296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix_hyper = confusion_matrix(y_test, prediction_hyper)\nnp.set_printoptions(precision=2)\ncnf_matrix_hyper","metadata":{"execution":{"iopub.status.busy":"2022-10-12T09:42:59.766744Z","iopub.execute_input":"2022-10-12T09:42:59.767278Z","iopub.status.idle":"2022-10-12T09:42:59.779745Z","shell.execute_reply.started":"2022-10-12T09:42:59.767242Z","shell.execute_reply":"2022-10-12T09:42:59.778713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. FINAL CONCLUSION\n***","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:16px;\">\nAs you can see, just by using one HyperParameter we have significantly increase out accuracy from 78 to 83.\n\nIn these dataset, first I perform some basic analysis and visualisation to understand the behaviour of the dataset.\n\nThen perform some preprocessing, which is deleteing unwanted columns, converting categorical variable into numerical, performing train test split and at last standarisation.\n\nThen at the end I train out Decision Tree Model twice, one without HyperParameter and then by using HyperParameter.\n\nI notice that Model with hyperparameter perform much better than the model which was trained without any hyperparameter.\n</span>","metadata":{}}]}